{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RqdbjrwsfmY"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import random\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from cvxpy import *\n",
        "from scipy import stats\n",
        "from scipy import sparse\n",
        "from scipy import linalg\n",
        "from keras.datasets import mnist\n",
        "from scipy.linalg import eigh\n",
        "from scipy.sparse import linalg\n",
        "from scipy.sparse import csr_array\n",
        "from scipy.sparse import coo_array\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse.linalg import eigs, eigsh\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.cluster import adjusted_rand_score\n",
        "from sklearn.metrics.pairwise import euclidean_distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0mWC5K12qgk"
      },
      "outputs": [],
      "source": [
        "def sigma_mat(dist, k):\n",
        "  dist.sort(axis=1)\n",
        "  dist_vec = np.array([dist[:, k-1]])\n",
        "  sigma_mat = 2 * np.outer(dist_vec, dist_vec)\n",
        "  return sigma_mat\n",
        "\n",
        "\n",
        "def DM_coordinates(v1, v2, k):\n",
        "  distsI = euclidean_distances(v1, v1)\n",
        "  dist_matI = np.copy(distsI)\n",
        "  sigma_matI = sigma_mat(dist_matI, k)\n",
        "\n",
        "  KI = np.exp(-distsI**2 / sigma_matI)\n",
        "  KI = KI - np.diag(np.diag(KI))\n",
        "  DI = np.diag(np.sum(KI, axis=1))\n",
        "  D_leftI = np.diag(np.sum(KI,axis = 1)**-0.5)\n",
        "  D_rightI = np.diag(np.sum(KI,axis = 1)**0.5)\n",
        "  rand_walk_LI = np.matmul(np.linalg.inv(DI), KI)\n",
        "  RWL_primeI = np.matmul(D_rightI, np.matmul(rand_walk_LI, D_leftI))\n",
        "\n",
        "  eigenValuesI, eigenVectorsI = eigh(RWL_primeI)\n",
        "  idxI = eigenValuesI.argsort()[::-1]\n",
        "  d_lamI = np.diag(eigenValuesI[idxI])\n",
        "  eig_I = D_leftI@eigenVectorsI[:, idxI]\n",
        "\n",
        "  diffusion_coordinatesI = np.matmul(eig_I, d_lamI)\n",
        "  diffusion_coordinatesI_df = pd.DataFrame(diffusion_coordinatesI)\n",
        "\n",
        "  distsII = euclidean_distances(v2, v2)\n",
        "  dist_matII = np.copy(distsII)\n",
        "  sigma_matII = sigma_mat(dist_matII, k)\n",
        "\n",
        "  KII = np.exp(-distsII**2 / sigma_matII)\n",
        "  KII = KII - np.diag(np.diag(KII))\n",
        "  DII = np.diag(np.sum(KII, axis=1))\n",
        "  D_leftII = np.diag(np.sum(KII,axis = 1)**-0.5)\n",
        "  D_rightII = np.diag(np.sum(KII,axis = 1)**0.5)\n",
        "  rand_walk_LII = np.matmul(np.linalg.inv(DII), KII)\n",
        "  RWL_primeII = np.matmul(D_rightII, np.matmul(rand_walk_LII, D_leftII))\n",
        "\n",
        "  eigenValuesII, eigenVectorsII = eigh(RWL_primeII)\n",
        "  idxII = eigenValuesII.argsort()[::-1]\n",
        "  d_lamII = np.diag(eigenValuesII[idxII])\n",
        "  eig_II = D_leftII@eigenVectorsII[:, idxII]\n",
        "\n",
        "  diffusion_coordinatesII = np.matmul(eig_II, d_lamII)\n",
        "  diffusion_coordinatesII_df = pd.DataFrame(diffusion_coordinatesII)\n",
        "  return diffusion_coordinatesI_df, diffusion_coordinatesII_df\n",
        "\n",
        "\n",
        "def check_conflicts(n, eigenvec):\n",
        "  if eigenvec.shape[1] == 1:\n",
        "    x_star = eigenvec\n",
        "  else:\n",
        "    d = eigenvec.shape[1]\n",
        "    x_star = eigenvec[:, (d - 1)]\n",
        "\n",
        "  list_x = range(1, n+1)\n",
        "  list_y = range(1, n+1)\n",
        "\n",
        "  pairs = []\n",
        "  for x in list_x:\n",
        "    for y in list_y:\n",
        "      pairs.append((x, y))\n",
        "\n",
        "  df = pd.DataFrame(pairs)\n",
        "  df[\"x_star\"] = x_star\n",
        "  df.columns = [\"row_idx\", \"col_idx\", \"x_star\"]\n",
        "  df_new = df\n",
        "\n",
        "  first_point = []\n",
        "  second_point = []\n",
        "\n",
        "  for i in range(n):\n",
        "    idx = np.argmax(df_new.x_star)\n",
        "    r_idx = df_new.iloc[idx, 0]\n",
        "    first_point.append(r_idx)\n",
        "    c_idx = df_new.iloc[idx, 1]\n",
        "    second_point.append(c_idx)\n",
        "    df_new = df_new[df_new.row_idx != r_idx]\n",
        "    df_new = df_new[df_new.col_idx != c_idx]\n",
        "\n",
        "  one_to_one = pd.DataFrame(first_point)\n",
        "  one_to_one[\"second_point\"] = second_point\n",
        "  one_to_one.columns = [\"first_point\", \"second_point\"]\n",
        "  return one_to_one\n",
        "\n",
        "\n",
        "def ARI_rate(pairs, n1):\n",
        "  idx1 = pairs.iloc[:,0]\n",
        "  vec = idx1 - 1\n",
        "  vec1 = np.where(vec <= n1-1, 0, vec)\n",
        "  vec1 = np.where(vec1 >= n1, 1, vec1)\n",
        "\n",
        "  idx2 = pairs.iloc[:,1]\n",
        "  vec = idx2 - 1\n",
        "  vec2 = np.where(vec <= n1-1, 0, vec)\n",
        "  vec2 = np.where(vec2 >= n1, 1, vec2)\n",
        "\n",
        "  return adjusted_rand_score(vec1, vec2)\n",
        "\n",
        "\n",
        "\n",
        "def kronecker_product2(A, B, n):\n",
        "  a_w = np.where(A != 0)\n",
        "  b_w = np.where(B != 0)\n",
        "  r_a = list(a_w[0])\n",
        "  c_a = list(a_w[1])\n",
        "  r_b = list(b_w[0])\n",
        "  c_b = list(b_w[1])\n",
        "\n",
        "  R1 = (np.outer(r_a, np.ones((1, len(r_b)))) * n).flatten() + (np.outer(np.ones((len(r_a), 1)), r_b)).flatten()\n",
        "  C1 = (np.outer(c_a, np.ones((1, len(c_b)))) * n).flatten() + (np.outer(np.ones((len(c_a), 1)), c_b)).flatten()\n",
        "  V1 = (np.outer(A[r_a, c_a].flatten(), np.ones((1, len(r_b))))) * (np.outer(np.ones((len(c_a), 1)), B[r_b, c_b].flatten()))\n",
        "  res = coo_array((V1.flatten(), (R1, C1)), shape = (n*n, n*n))\n",
        "  return res\n",
        "\n",
        "\n",
        "def compute_M2(X, Y, k):\n",
        "  x_dist = euclidean_distances(X,X)\n",
        "  idx_x = np.argpartition(x_dist, range(k))[:, k:]\n",
        "  x_dist[np.arange(idx_x.shape[0])[:,None], idx_x] = 2\n",
        "  x_dist = 2 - x_dist\n",
        "  x_dist = 0.5*(x_dist+x_dist.T)\n",
        "\n",
        "  y_dist = euclidean_distances(Y,Y)\n",
        "  idx_y = np.argpartition(y_dist, range(k))[:, k:]\n",
        "  y_dist[np.arange(idx_y.shape[0])[:,None], idx_y] = 2\n",
        "  y_dist = 2 - y_dist\n",
        "  y_dist = 0.5*(y_dist+y_dist.T)\n",
        "\n",
        "  n = x_dist.shape[0]\n",
        "  M2 = kronecker_product2(x_dist, y_dist, n)\n",
        "  return M2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmzLG7XL23hp"
      },
      "outputs": [],
      "source": [
        "def compute_M2_new(X, Y, k):\n",
        "  x_dist = euclidean_distances(X,X)\n",
        "  idx_x = np.argpartition(x_dist, range(k))[:, k:]\n",
        "  x_dist[np.arange(idx_x.shape[0])[:,None], idx_x] = 2\n",
        "  x_dist = 2 - x_dist\n",
        "  Q_x = 0.5*(x_dist+x_dist.T)\n",
        "\n",
        "  y_dist = euclidean_distances(Y,Y)\n",
        "  idx_y = np.argpartition(y_dist, range(k))[:, k:]\n",
        "  y_dist[np.arange(idx_y.shape[0])[:,None], idx_y] = 2\n",
        "  y_dist = 2 - y_dist\n",
        "  Q_y = 0.5*(y_dist+y_dist.T)\n",
        "\n",
        "  return Q_x, Q_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfjKTYlj23eQ",
        "outputId": "25625104-37bd-4181-dcd0-52639b4c8635"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "y_train = y_train.reshape(60000, 1)\n",
        "\n",
        "idx = np.random.choice(X_train.shape[0], 50000, replace=False)\n",
        "mnist_data = X_train[idx]\n",
        "mnist_data = pd.DataFrame(mnist_data)\n",
        "\n",
        "mnist_lab = y_train[idx]\n",
        "mnist_lab = pd.DataFrame(mnist_lab)\n",
        "mnist_lab.columns = ['lable']\n",
        "\n",
        "dat = pd.concat([mnist_data, mnist_lab], axis=1)\n",
        "\n",
        "\n",
        "dat0 = dat.loc[dat['lable'] == 0]\n",
        "dat1 = dat.loc[dat['lable'] == 1]\n",
        "dat2 = dat.loc[dat['lable'] == 2]\n",
        "dat3 = dat.loc[dat['lable'] == 3]\n",
        "dat4 = dat.loc[dat['lable'] == 4]\n",
        "dat5 = dat.loc[dat['lable'] == 5]\n",
        "dat6 = dat.loc[dat['lable'] == 6]\n",
        "dat7 = dat.loc[dat['lable'] == 7]\n",
        "dat8 = dat.loc[dat['lable'] == 8]\n",
        "dat9 = dat.loc[dat['lable'] == 9]\n",
        "\n",
        "\n",
        "def data_generator_MNIST(n, datA, datB):\n",
        "  p_mnist = 784\n",
        "\n",
        "  v1 = datA.sample(n = n)\n",
        "  v2 = datB.sample(n = n)\n",
        "  datA_ = pd.concat([v1.iloc[:(int(n/2)),:], v2.iloc[:(int(n/2)),:]])\n",
        "  datB_ = pd.concat([v1.iloc[(int(n/2)):,:], v2.iloc[(int(n/2)):,:]])\n",
        "\n",
        "  X1_mnist = datA_.iloc[:, 0:p_mnist]\n",
        "  X2_mnist = datB_.iloc[:, 0:p_mnist]\n",
        "  Y1_mnist = datA_.iloc[:, p_mnist]\n",
        "  Y2_mnist = datB_.iloc[:, p_mnist]\n",
        "  return X1_mnist, X2_mnist, Y1_mnist, Y2_mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZF8-EAy23a0"
      },
      "outputs": [],
      "source": [
        "def matching_rate_M2_mixed_CC(n, k, num_coords, M, data_func, *args):\n",
        "  st = time.time()\n",
        "  n1 = int(n/2)\n",
        "\n",
        "  num_ev = []\n",
        "  ARI_lst = []\n",
        "  ARI_lst1 = []\n",
        "  ARI_lst2 = []\n",
        "  ARI_lst3 = []\n",
        "  ARI_lst4 = []\n",
        "  ARI_lst5 = []\n",
        "  ARI_lst6 = []\n",
        "  ARI_lst7 = []\n",
        "  ARI_lst8 = []\n",
        "  ARI_lst9 = []\n",
        "\n",
        "  for i in range(M):\n",
        "    print(i)\n",
        "    view_1, view_2, lab1, lab2 = data_func(*args)\n",
        "\n",
        "    diffusion_coordinatesI_df, diffusion_coordinatesII_df = DM_coordinates(view_1, view_2, k)\n",
        "\n",
        "    leadEV1 = diffusion_coordinatesI_df.iloc[:,1:num_coords]\n",
        "    leadEV2 = diffusion_coordinatesII_df.iloc[:,1:num_coords]\n",
        "\n",
        "    norm_view1 = normalize(leadEV1, axis=1, norm='l2')\n",
        "    norm_view2 = normalize(leadEV2, axis=1, norm='l2')\n",
        "\n",
        "    Q_1, Q_2 = compute_M2_new(norm_view1, norm_view2, k)\n",
        "    e_val_Q1, e_vec_Q1 = eigsh(Q_1, k=3)\n",
        "    e_val_Q2, e_vec_Q2 = eigsh(Q_2, k=3)\n",
        "\n",
        "    V = []\n",
        "    for i in range(3):\n",
        "      for j in range(3):\n",
        "        V.append(np.reshape(np.outer(e_vec_Q1[:,i],e_vec_Q2[:,j]),(n**2)))\n",
        "\n",
        "    eVector1 = V[8]\n",
        "    eVector2 = V[7]\n",
        "    eVector3 = V[6]\n",
        "    eVector4 = V[5]\n",
        "    eVector5 = V[4]\n",
        "    eVector6 = V[3]\n",
        "    eVector7 = V[2]\n",
        "    eVector8 = V[1]\n",
        "    eVector9 = V[0]\n",
        "\n",
        "    pairs1 = check_conflicts(n, eVector1.reshape(n*n, 1))\n",
        "    pairs2 = check_conflicts(n, eVector2.reshape(n*n, 1))\n",
        "    pairs3 = check_conflicts(n, eVector3.reshape(n*n, 1))\n",
        "    pairs4 = check_conflicts(n, eVector4.reshape(n*n, 1))\n",
        "    pairs5 = check_conflicts(n, eVector5.reshape(n*n, 1))\n",
        "    pairs6 = check_conflicts(n, eVector6.reshape(n*n, 1))\n",
        "    pairs7 = check_conflicts(n, eVector7.reshape(n*n, 1))\n",
        "    pairs8 = check_conflicts(n, eVector8.reshape(n*n, 1))\n",
        "    pairs9 = check_conflicts(n, eVector9.reshape(n*n, 1))\n",
        "\n",
        "    a1 = ARI_rate(pairs1, n1)\n",
        "    ARI_lst1.append(a1)\n",
        "    a2 = ARI_rate(pairs2, n1)\n",
        "    ARI_lst2.append(a2)\n",
        "    a3 = ARI_rate(pairs3, n1)\n",
        "    ARI_lst3.append(a3)\n",
        "    a4 = ARI_rate(pairs4, n1)\n",
        "    ARI_lst4.append(a4)\n",
        "    a5 = ARI_rate(pairs5, n1)\n",
        "    ARI_lst5.append(a5)\n",
        "    a6 = ARI_rate(pairs6, n1)\n",
        "    ARI_lst6.append(a6)\n",
        "    a7 = ARI_rate(pairs7, n1)\n",
        "    ARI_lst7.append(a7)\n",
        "    a8 = ARI_rate(pairs8, n1)\n",
        "    ARI_lst8.append(a8)\n",
        "    a9 = ARI_rate(pairs9, n1)\n",
        "    ARI_lst9.append(a9)\n",
        "\n",
        "    v1 = np.zeros((n*n,1))\n",
        "    n_pairs1 = pairs1.sort_values('first_point')\n",
        "    for i in range(n):\n",
        "      d1 = n_pairs1.iloc[i,1]\n",
        "      idx1 = (n*i) + d1-1\n",
        "      v1[idx1] = 1\n",
        "    P_v1 = v1.reshape(n, n)\n",
        "    result_matrix1 = Q_2 @ P_v1 @ Q_1.T @ P_v1.T\n",
        "    obj_res1 = np.trace(result_matrix1)\n",
        "\n",
        "    v2 = np.zeros((n*n,1))\n",
        "    n_pairs2 = pairs2.sort_values('first_point')\n",
        "    for i in range(n):\n",
        "      d2 = n_pairs2.iloc[i,1]\n",
        "      idx2 = (n*i) + d2-1\n",
        "      v2[idx2] = 1\n",
        "    P_v2 = v2.reshape(n, n)\n",
        "    result_matrix2 = Q_2 @ P_v2 @ Q_1.T @ P_v2.T\n",
        "    obj_res2 = np.trace(result_matrix2)\n",
        "\n",
        "    v3 = np.zeros((n*n,1))\n",
        "    n_pairs3 = pairs3.sort_values('first_point')\n",
        "    for i in range(n):\n",
        "      d3 = n_pairs3.iloc[i,1]\n",
        "      idx3 = (n*i) + d3-1\n",
        "      v3[idx3] = 1\n",
        "    P_v3 = v3.reshape(n, n)\n",
        "    result_matrix3 = Q_2 @ P_v3 @ Q_1.T @ P_v3.T\n",
        "    obj_res3 = np.trace(result_matrix3)\n",
        "\n",
        "    v4 = np.zeros((n*n,1))\n",
        "    n_pairs4 = pairs4.sort_values('first_point')\n",
        "    for i in range(n):\n",
        "      d4 = n_pairs4.iloc[i,1]\n",
        "      idx4 = (n*i) + d4-1\n",
        "      v4[idx4] = 1\n",
        "    P_v4 = v4.reshape(n, n)\n",
        "    result_matrix4 = Q_2 @ P_v4 @ Q_1.T @ P_v4.T\n",
        "    obj_res4 = np.trace(result_matrix4)\n",
        "\n",
        "    v5 = np.zeros((n*n,1))\n",
        "    n_pairs5 = pairs5.sort_values('first_point')\n",
        "    for i in range(n):\n",
        "      d5 = n_pairs5.iloc[i,1]\n",
        "      idx5 = (n*i) + d5-1\n",
        "      v5[idx5] = 1\n",
        "    P_v5 = v5.reshape(n, n)\n",
        "    result_matrix5 = Q_2 @ P_v5 @ Q_1.T @ P_v5.T\n",
        "    obj_res5 = np.trace(result_matrix5)\n",
        "\n",
        "    v6 = np.zeros((n*n,1))\n",
        "    n_pairs6 = pairs6.sort_values('first_point')\n",
        "    for i in range(n):\n",
        "      d6 = n_pairs6.iloc[i,1]\n",
        "      idx6 = (n*i) + d6-1\n",
        "      v6[idx6] = 1\n",
        "    P_v6 = v6.reshape(n, n)\n",
        "    result_matrix6 = Q_2 @ P_v6 @ Q_1.T @ P_v6.T\n",
        "    obj_res6 = np.trace(result_matrix6)\n",
        "\n",
        "    v7 = np.zeros((n*n,1))\n",
        "    n_pairs7 = pairs7.sort_values('first_point')\n",
        "    for i in range(n):\n",
        "      d7 = n_pairs7.iloc[i,1]\n",
        "      idx7 = (n*i) + d7-1\n",
        "      v7[idx7] = 1\n",
        "    P_v7 = v7.reshape(n, n)\n",
        "    result_matrix7 = Q_2 @ P_v7 @ Q_1.T @ P_v7.T\n",
        "    obj_res7 = np.trace(result_matrix7)\n",
        "\n",
        "    v8 = np.zeros((n*n,1))\n",
        "    n_pairs8 = pairs8.sort_values('first_point')\n",
        "    for i in range(n):\n",
        "      d8 = n_pairs8.iloc[i,1]\n",
        "      idx8 = (n*i) + d8-1\n",
        "      v8[idx8] = 1\n",
        "    P_v8 = v8.reshape(n, n)\n",
        "    result_matrix8 = Q_2 @ P_v8 @ Q_1.T @ P_v8.T\n",
        "    obj_res8 = np.trace(result_matrix8)\n",
        "\n",
        "    v9 = np.zeros((n*n,1))\n",
        "    n_pairs9 = pairs9.sort_values('first_point')\n",
        "    for i in range(n):\n",
        "      d9 = n_pairs9.iloc[i,1]\n",
        "      idx9 = (n*i) + d9-1\n",
        "      v9[idx9] = 1\n",
        "    P_v9 = v9.reshape(n, n)\n",
        "    result_matrix9 = Q_2 @ P_v9 @ Q_1.T @ P_v9.T\n",
        "    obj_res9 = np.trace(result_matrix9)\n",
        "\n",
        "    obj_arr = np.array([float(obj_res1), float(obj_res2), float(obj_res3), float(obj_res4), float(obj_res5), float(obj_res6), float(obj_res7), float(obj_res8), float(obj_res9)])\n",
        "    idx = np.argmax(obj_arr)\n",
        "    max = np.max(obj_arr)\n",
        "    print(\"The result - ev number \" + str((idx+1)) + \", objective function = \" + str(round(max, 3)))\n",
        "\n",
        "    a_arr = np.array([float(a1), float(a2), float(a3), float(a4), float(a5), float(a6), float(a7), float(a8), float(a9)])\n",
        "    ARI_lst.append(a_arr[idx])\n",
        "    num_ev.append(idx+1)\n",
        "\n",
        "  et = time.time()\n",
        "\n",
        "  total_time = et - st\n",
        "  print(\"Execution time of matching_rate: \", total_time, \" seconds\")\n",
        "\n",
        "  return pairs1, pairs2, pairs3, pairs4, pairs5, pairs6, pairs7, pairs8, pairs9, view_1, view_2, ARI_lst1, ARI_lst2, ARI_lst3, ARI_lst4, ARI_lst5, ARI_lst6, ARI_lst7, ARI_lst8, ARI_lst9, num_ev, ARI_lst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0k5BTJEW23Xo"
      },
      "outputs": [],
      "source": [
        "n_lst = [50, 60, 70, 80, 90, 100, 120, 150]\n",
        "l = len(n_lst)\n",
        "num_coords = 2\n",
        "\n",
        "st = time.time()\n",
        "\n",
        "def GM_sys_M2_mixed_CC(ns, M, num_coords):\n",
        "  num_nodes = []\n",
        "  run_time = []\n",
        "  num_evs = []\n",
        "  ARI_lsts = []\n",
        "\n",
        "  median_ARI_M = []\n",
        "  median_ARI_M_1 = []\n",
        "  median_ARI_M_2 = []\n",
        "  median_ARI_M_3 = []\n",
        "\n",
        "  mean_ARI_M = []\n",
        "  mean_ARI_M_1 = []\n",
        "  mean_ARI_M_2 = []\n",
        "  mean_ARI_M_3 = []\n",
        "\n",
        "  std_ARI_M = []\n",
        "  std_ARI_M_1 = []\n",
        "  std_ARI_M_2 = []\n",
        "  std_ARI_M_3 = []\n",
        "\n",
        "  for i in range(l):\n",
        "    print(i)\n",
        "    num_nodes.append(ns[i])\n",
        "    st = time.time()\n",
        "    k_neigh = 20\n",
        "\n",
        "    pairs1, pairs2, pairs3, pairs4, pairs5, pairs6, pairs7, pairs8, pairs9, view_1, view_2, ARI_lst1, ARI_lst2, ARI_lst3, ARI_lst4, ARI_lst5, ARI_lst6, ARI_lst7, ARI_lst8, ARI_lst9, num_ev, ARI_lst = matching_rate_M2_mixed_CC(ns[i], k_neigh, num_coords, M, data_generator_MNIST, ns[i], dat0, dat1)\n",
        "    et = time.time()\n",
        "    total_time = et - st\n",
        "    run_time.append(total_time)\n",
        "    num_evs.append(num_ev)\n",
        "    ARI_lsts.append(ARI_lst)\n",
        "\n",
        "    # M\n",
        "    median_ARI_M.append(np.median(np.abs(ARI_lst)))\n",
        "    mean_ARI_M.append(np.mean(np.abs(ARI_lst)))\n",
        "    std_ARI_M.append(np.std(np.abs(ARI_lst)))\n",
        "\n",
        "    median_ARI_M_1.append(np.median(np.abs(ARI_lst1)))\n",
        "    mean_ARI_M_1.append(np.mean(np.abs(ARI_lst1)))\n",
        "    std_ARI_M_1.append(np.std(np.abs(ARI_lst1)))\n",
        "\n",
        "    median_ARI_M_2.append(np.median(np.abs(ARI_lst2)))\n",
        "    mean_ARI_M_2.append(np.mean(np.abs(ARI_lst2)))\n",
        "    std_ARI_M_2.append(np.std(np.abs(ARI_lst2)))\n",
        "\n",
        "    median_ARI_M_3.append(np.median(np.abs(ARI_lst3)))\n",
        "    mean_ARI_M_3.append(np.mean(np.abs(ARI_lst3)))\n",
        "    std_ARI_M_3.append(np.std(np.abs(ARI_lst3)))\n",
        "\n",
        "  return num_nodes, run_time, median_ARI_M_1, median_ARI_M_2, median_ARI_M_3, mean_ARI_M_1, mean_ARI_M_2, mean_ARI_M_3, std_ARI_M_1, std_ARI_M_2, std_ARI_M_3, num_evs, median_ARI_M, mean_ARI_M, std_ARI_M, ARI_lsts\n",
        "\n",
        "et = time.time()\n",
        "total_time = et - st\n",
        "print(\"Execution time: \", total_time, \" seconds\")\n",
        "\n",
        "m = 25\n",
        "num_nodes, run_time, median_ARI_M_1, median_ARI_M_2, median_ARI_M_3, mean_ARI_M_1, mean_ARI_M_2, mean_ARI_M_3, std_ARI_M_1, std_ARI_M_2, std_ARI_M_3, num_evs, median_ARI_M, mean_ARI_M, std_ARI_M, ARI_lsts = GM_sys_M2_mixed_CC(n_lst, m, num_coords)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}